#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æª¢æŸ¥ä¸¦æ¸…ç† colne_icol_images/images.json ä¸­çš„ 404 åœ–ç‰‡
"""

import json
import requests
import os
from datetime import datetime
import time

def check_image_availability(base_url, filename, timeout=10):
    """æª¢æŸ¥åœ–ç‰‡æ˜¯å¦å¯ç”¨"""
    full_url = f"{base_url}/colne_icol_images/{filename}"
    
    try:
        response = requests.head(full_url, timeout=timeout)
        return response.status_code == 200, response.status_code
    except requests.exceptions.RequestException as e:
        print(f"   âŒ ç¶²è·¯éŒ¯èª¤: {e}")
        return False, "Network Error"

def check_local_file_exists(filename):
    """æª¢æŸ¥æœ¬åœ°æª”æ¡ˆæ˜¯å¦å­˜åœ¨"""
    file_path = os.path.join("colne_icol_images", filename)
    return os.path.exists(file_path)

def clean_images_json():
    """æ¸…ç† images.json ä¸­çš„ç„¡æ•ˆåœ–ç‰‡"""
    json_path = "colne_icol_images/images.json"
    
    # è®€å–ç¾æœ‰çš„ JSON è³‡æ–™
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            images_data = json.load(f)
    except FileNotFoundError:
        print(f"âŒ æ‰¾ä¸åˆ°æª”æ¡ˆ: {json_path}")
        return
    except json.JSONDecodeError as e:
        print(f"âŒ JSON æ ¼å¼éŒ¯èª¤: {e}")
        return
    
    print(f"ğŸ“Š é–‹å§‹æª¢æŸ¥ {len(images_data)} å¼µåœ–ç‰‡...")
    print("=" * 60)
    
    # GitHub Pages åŸºç¤ URL
    github_pages_base = "https://future801113.github.io/i-love-col"
    
    valid_images = []
    invalid_images = []
    
    for i, image_info in enumerate(images_data, 1):
        filename = image_info.get("filename", "")
        
        print(f"[{i:3d}/{len(images_data)}] æª¢æŸ¥: {filename}")
        
        # æª¢æŸ¥æœ¬åœ°æª”æ¡ˆæ˜¯å¦å­˜åœ¨
        local_exists = check_local_file_exists(filename)
        print(f"   ğŸ“ æœ¬åœ°æª”æ¡ˆ: {'å­˜åœ¨' if local_exists else 'ä¸å­˜åœ¨'}")
        
        # æª¢æŸ¥ç·šä¸Šåœ–ç‰‡æ˜¯å¦å¯ç”¨
        is_available, status_code = check_image_availability(github_pages_base, filename)
        print(f"   ğŸŒ ç·šä¸Šç‹€æ…‹: {status_code} ({'å¯ç”¨' if is_available else 'ä¸å¯ç”¨'})")
        
        # å¦‚æœæœ¬åœ°æª”æ¡ˆå­˜åœ¨æˆ–ç·šä¸Šåœ–ç‰‡å¯ç”¨ï¼Œå‰‡ä¿ç•™
        if local_exists or is_available:
            valid_images.append(image_info)
            print(f"   âœ… ä¿ç•™")
        else:
            invalid_images.append(image_info)
            print(f"   âŒ ç§»é™¤")
        
        print()
        
        # é¿å…è«‹æ±‚éæ–¼é »ç¹
        time.sleep(0.5)
    
    print("=" * 60)
    print(f"ğŸ“Š æª¢æŸ¥çµæœ:")
    print(f"   âœ… æœ‰æ•ˆåœ–ç‰‡: {len(valid_images)} å¼µ")
    print(f"   âŒ ç„¡æ•ˆåœ–ç‰‡: {len(invalid_images)} å¼µ")
    
    if invalid_images:
        print(f"\nğŸ—‘ï¸ å°‡ç§»é™¤çš„åœ–ç‰‡:")
        for img in invalid_images:
            print(f"   - {img.get('filename', 'Unknown')}")
    
    # å¦‚æœæœ‰ç„¡æ•ˆåœ–ç‰‡ï¼Œæ›´æ–° JSON æª”æ¡ˆ
    if invalid_images:
        # å‚™ä»½åŸå§‹æª”æ¡ˆ
        backup_path = f"{json_path}.backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        with open(backup_path, 'w', encoding='utf-8') as f:
            json.dump(images_data, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ åŸå§‹æª”æ¡ˆå·²å‚™ä»½è‡³: {backup_path}")
        
        # å¯«å…¥æ¸…ç†å¾Œçš„è³‡æ–™
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(valid_images, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… å·²æ›´æ–° {json_path}ï¼Œç§»é™¤äº† {len(invalid_images)} å¼µç„¡æ•ˆåœ–ç‰‡")
    else:
        print(f"\nğŸ‰ æ‰€æœ‰åœ–ç‰‡éƒ½æ˜¯æœ‰æ•ˆçš„ï¼Œç„¡éœ€æ¸…ç†ï¼")

def update_scraper_to_exclude_domain():
    """æ›´æ–°çˆ¬èŸ²è…³æœ¬ï¼Œæ’é™¤æŒ‡å®šçš„ domain"""
    scraper_path = "scripts/web_scraper_simple.py"
    
    if not os.path.exists(scraper_path):
        print(f"âŒ æ‰¾ä¸åˆ°çˆ¬èŸ²è…³æœ¬: {scraper_path}")
        return
    
    print(f"\nğŸ”§ æª¢æŸ¥çˆ¬èŸ²è…³æœ¬æ˜¯å¦éœ€è¦æ›´æ–°...")
    
    # è®€å–ç¾æœ‰è…³æœ¬
    with open(scraper_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æª¢æŸ¥æ˜¯å¦å·²ç¶“æœ‰æ’é™¤é‚è¼¯
    excluded_domain = "https://future801113.github.io/i-love-col/"
    
    if excluded_domain in content:
        print(f"âœ… çˆ¬èŸ²è…³æœ¬å·²ç¶“åŒ…å«æ’é™¤é‚è¼¯")
        return
    
    # åœ¨ download_image æ–¹æ³•ä¸­æ·»åŠ  domain æª¢æŸ¥
    old_download_method = '''def download_image(self, url, filename):
        """ä¸‹è¼‰åœ–ç‰‡"""
        try:
            headers = {'''
    
    new_download_method = f'''def download_image(self, url, filename):
        """ä¸‹è¼‰åœ–ç‰‡"""
        try:
            # æ’é™¤æŒ‡å®šçš„ domain
            if url.startswith("{excluded_domain}"):
                print(f"   â­ï¸  è·³éæ’é™¤çš„ domain: {{url[:80]}}...")
                return None
            
            headers = {{'''
    
    if old_download_method in content:
        updated_content = content.replace(old_download_method, new_download_method)
        
        # å‚™ä»½åŸå§‹æª”æ¡ˆ
        backup_path = f"{scraper_path}.backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        with open(backup_path, 'w', encoding='utf-8') as f:
            f.write(content)
        print(f"ğŸ’¾ çˆ¬èŸ²è…³æœ¬å·²å‚™ä»½è‡³: {backup_path}")
        
        # å¯«å…¥æ›´æ–°å¾Œçš„å…§å®¹
        with open(scraper_path, 'w', encoding='utf-8') as f:
            f.write(updated_content)
        
        print(f"âœ… å·²æ›´æ–°çˆ¬èŸ²è…³æœ¬ï¼Œæ·»åŠ äº† domain æ’é™¤é‚è¼¯")
    else:
        print(f"âš ï¸ ç„¡æ³•æ‰¾åˆ°é æœŸçš„æ–¹æ³•ç°½åï¼Œè«‹æ‰‹å‹•æ·»åŠ æ’é™¤é‚è¼¯")

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ§¹ é–‹å§‹æ¸…ç† colne_icol_images/images.json...")
    print("ğŸ¯ ç›®æ¨™: ç§»é™¤ 404 åœ–ç‰‡ï¼Œæ’é™¤æŒ‡å®š domain")
    print("=" * 60)
    
    # æ­¥é©Ÿ 1: æ¸…ç† JSON ä¸­çš„ç„¡æ•ˆåœ–ç‰‡
    clean_images_json()
    
    # æ­¥é©Ÿ 2: æ›´æ–°çˆ¬èŸ²è…³æœ¬æ’é™¤æŒ‡å®š domain
    update_scraper_to_exclude_domain()
    
    print("\nğŸ‰ æ¸…ç†å®Œæˆï¼")

if __name__ == "__main__":
    main()