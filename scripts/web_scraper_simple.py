#!/usr/bin/env python3
"""
ç°¡åŒ–ç‰ˆ Twitter åœ–ç‰‡çˆ¬èŸ² - GitHub Actions ç‰ˆæœ¬
ä½¿ç”¨å…¬é–‹çš„ç”¨æˆ¶é é¢è€Œä¸æ˜¯æœå°‹é é¢
"""

import os
import sys
import json
import time
import random
import requests
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from selenium.webdriver.common.keys import Keys

class TwitterImageScraperSimple:
    def __init__(self, config_file='config.json', username=None):
        self.config_file = config_file
        self.config = self.load_config()
        self.username = username
        self.images_dir = self.get_images_directory(username)
        self.setup_directories()
        self.driver = None
        
        # LINE Bot è¨­å®š - æ”¯æ´ç’°å¢ƒè®Šæ•¸
        self.line_config = self.config.get('line_bot', {})
        self.channel_access_token = (
            os.environ.get('LINE_CHANNEL_ACCESS_TOKEN') or 
            self.line_config.get('channel_access_token', '')
        )
        self.target_id = (
            os.environ.get('LINE_TARGET_ID') or 
            self.line_config.get('target_id', '')
        )
        self.github_pages_base = self.line_config.get('github_pages_base', 'https://future801113.github.io/i-love-col')
    
    def get_images_directory(self, username):
        """æ ¹æ“šä½¿ç”¨è€…åç¨±æ±ºå®šåœ–ç‰‡å­˜æ”¾ç›®éŒ„ - GitHub Actions ç‰ˆæœ¬"""
        if username == 'colne_icol':
            return self.config.get('colne_icol_directory', '../colne_icol_images')
        else:
            return self.config.get('images_directory', '../images')
        
    def load_config(self):
        """è¼‰å…¥è¨­å®šæª”"""
        try:
            with open(self.config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"âŒ æ‰¾ä¸åˆ°è¨­å®šæª”: {self.config_file}")
            # åœ¨ GitHub Actions ç’°å¢ƒä¸­ï¼Œæä¾›é»˜èªè¨­å®š
            return {
                "images_directory": "../images",
                "colne_icol_directory": "../colne_icol_images",
                "line_bot": {
                    "github_pages_base": "https://future801113.github.io/i-love-col"
                }
            }
        except json.JSONDecodeError as e:
            print(f"âŒ è¨­å®šæª”æ ¼å¼éŒ¯èª¤: {e}")
            return {}
    
    def setup_directories(self):
        """å»ºç«‹å¿…è¦çš„ç›®éŒ„"""
        if not os.path.exists(self.images_dir):
            os.makedirs(self.images_dir)
            print(f"ğŸ“ å»ºç«‹åœ–ç‰‡ç›®éŒ„: {self.images_dir}")
    
    def load_blacklist(self):
        """è¼‰å…¥é»‘åå–®æª”æ¡ˆ"""
        try:
            blacklist_path = os.path.join(os.path.dirname(self.images_dir), 'images', 'blacklist.json')
            if os.path.exists(blacklist_path):
                with open(blacklist_path, 'r', encoding='utf-8') as f:
                    blacklist_data = json.load(f)
                    return {item['filename'] for item in blacklist_data}
            return set()
        except Exception as e:
            print(f"âš ï¸  è¼‰å…¥é»‘åå–®æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return set()
    
    def is_filename_blacklisted(self, filename):
        """æª¢æŸ¥æª”åæ˜¯å¦åœ¨é»‘åå–®ä¸­"""
        blacklist = self.load_blacklist()
        return filename in blacklist
    
    def setup_driver(self):
        """è¨­å®š Chrome ç€è¦½å™¨é©…å‹• - å¼·åŒ–åçˆ¬èŸ²å°æŠ—"""
        chrome_options = Options()
        # GitHub Actions å¿…éœ€çš„é¸é …
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--disable-software-rasterizer')
        chrome_options.add_argument('--window-size=1920,1080')
        
        # åçˆ¬èŸ²å°æŠ— - éš±è—è‡ªå‹•åŒ–ç‰¹å¾µ
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        chrome_options.add_argument('--disable-extensions')
        
        # ä½¿ç”¨çœŸå¯¦ç”¨æˆ¶ä»£ç†ï¼ˆWindows + Chromeï¼‰
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36')
        
        # æ·»åŠ æ›´å¤šç€è¦½å™¨ç‰¹å¾µä½¿å…¶çœ‹èµ·ä¾†æ›´åƒçœŸå¯¦ç”¨æˆ¶
        chrome_options.add_argument('--disable-plugins')
        chrome_options.add_argument('--disable-images')  # åŠ é€Ÿé é¢è¼‰å…¥
        chrome_options.add_argument('--no-first-run')
        chrome_options.add_argument('--no-default-browser-check')
        
        try:
            self.driver = webdriver.Chrome(options=chrome_options)
            
            # è¨­å®šå¤šå€‹éš±è— webdriver çš„æ–¹å¼
            self.driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
                'source': '''
                    Object.defineProperty(navigator, 'webdriver', {
                        get: () => undefined
                    });
                    Object.defineProperty(navigator, 'plugins', {
                        get: () => [1, 2, 3, 4, 5]
                    });
                    Object.defineProperty(navigator, 'languages', {
                        get: () => ['zh-TW', 'zh', 'en']
                    });
                '''
            })
            
            # æ·»åŠ  headers æ¨¡ä»¿çœŸå¯¦ç€è¦½å™¨
            self.driver.execute_cdp_cmd('Network.setUserAgentOverride', {
                "userAgent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36"
            })
            
            print("âœ… Chrome ç€è¦½å™¨é©…å‹•åˆå§‹åŒ–æˆåŠŸï¼ˆå¼·åŒ–åçˆ¬èŸ²æ¨¡å¼ï¼‰")
            return True
        except Exception as e:
            print(f"âŒ Chrome ç€è¦½å™¨é©…å‹•åˆå§‹åŒ–å¤±æ•—: {e}")
            return False
    
    def build_nitter_search_url(self, username, start_date=None, end_date=None, nitter_instance="nitter.poast.org"):
        """å»ºç«‹ Nitter æœå°‹ URLï¼ˆæ”¯æ´æ—¥æœŸå€é–“ï¼‰"""
        import urllib.parse
        # å»ºç«‹æœå°‹æŸ¥è©¢
        if start_date and end_date:
            query = f"from:{username} since:{start_date} until:{end_date}"
        elif start_date:
            query = f"from:{username} since:{start_date}"
        else:
            query = f"from:{username}"
        encoded_query = urllib.parse.quote(query)
        # åŠ ä¸Š f-media=on & e-nativeretweets=on åƒæ•¸
        return f"https://{nitter_instance}/search?q={encoded_query}&f=tweets&f-media=on&e-nativeretweets=on"
    
    def extract_image_urls(self):
        """æå–åœ–ç‰‡ URL"""
        image_urls = []
        
        try:
            print("ğŸ” é–‹å§‹æå–åœ–ç‰‡ URL...")
            
            # åˆ¤æ–·æ˜¯å¦ç‚º Nitter é é¢
            current_url = self.driver.current_url.lower()
            is_nitter = "nitter" in current_url
            
            if is_nitter:
                print("ğŸ¦ æª¢æ¸¬åˆ° Nitter é é¢ï¼Œä½¿ç”¨ Nitter é¸æ“‡å™¨")
                # Nitter å°ˆç”¨é¸æ“‡å™¨
                selectors = [
                    ".still-image img",
                    ".attachment img",
                    ".gallery-row img",
                    "a.still-image",
                ]
            else:
                print("ğŸ”µ ä½¿ç”¨ Twitter/X å®˜æ–¹é¸æ“‡å™¨")
                # Twitter/X å®˜æ–¹é¸æ“‡å™¨
                selectors = [
                    "img[src*='pbs.twimg.com'][src*='media']",
                    "div[data-testid='tweetPhoto'] img",
                    "[data-testid='tweetPhoto'] img",
                    "article img[src*='pbs.twimg.com']",
                    "img[alt*='Image']"
                ]
            
            for selector in selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    print(f"   ä½¿ç”¨é¸æ“‡å™¨ '{selector}' æ‰¾åˆ° {len(elements)} å€‹å…ƒç´ ")
                    
                    for element in elements:
                        try:
                            # å°æ–¼ Nitter çš„ a.still-image é€£çµï¼Œä½¿ç”¨ href å±¬æ€§
                            if is_nitter and selector == "a.still-image":
                                href = element.get_attribute("href")
                                if href and "/pic/orig/" in href:
                                    if "media%2F" in href:
                                        media_id = href.split("media%2F")[1].split(".")[0]
                                        large_url = f"https://pbs.twimg.com/media/{media_id}?format=jpg&name=large"
                                        if large_url not in image_urls:
                                            image_urls.append(large_url)
                                            print(f"   âœ… æ‰¾åˆ°åœ–ç‰‡ (from href): {large_url[:80]}...")
                            else:
                                # ä¸€èˆ¬çš„ img å…ƒç´ 
                                src = element.get_attribute("src")
                                if src:
                                    if is_nitter:
                                        if "/pic/media%2F" in src:
                                            media_id = src.split("media%2F")[1].split(".")[0].split("%")[0]
                                            large_url = f"https://pbs.twimg.com/media/{media_id}?format=jpg&name=large"
                                            if large_url not in image_urls:
                                                image_urls.append(large_url)
                                                print(f"   âœ… æ‰¾åˆ°åœ–ç‰‡ (from src): {large_url[:80]}...")
                                    else:
                                        # Twitter/X å®˜æ–¹è™•ç†
                                        if ("pbs.twimg.com" in src or "pic.twitter.com" in src):
                                            if "?format=" in src or "&name=" in src:
                                                base_url = src.split("?")[0]
                                                large_url = f"{base_url}?format=jpg&name=large"
                                            else:
                                                large_url = src
                                            
                                            if large_url not in image_urls:
                                                image_urls.append(large_url)
                                                print(f"   âœ… æ‰¾åˆ°åœ–ç‰‡: {large_url[:80]}...")
                        except Exception as e:
                            continue
                except Exception as e:
                    print(f"   é¸æ“‡å™¨åŸ·è¡Œå¤±æ•—: {e}")
                    continue
            
            # å»é‡ä¸¦é™åˆ¶æ•¸é‡
            unique_urls = list(dict.fromkeys(image_urls))
            print(f"ğŸ–¼ï¸  ç¸½å…±æ‰¾åˆ° {len(unique_urls)} å¼µå”¯ä¸€åœ–ç‰‡")
            
            return unique_urls
            
        except Exception as e:
            print(f"âŒ æå–åœ–ç‰‡ URL æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return []
    
    def download_image(self, url, filename):
        """ä¸‹è¼‰åœ–ç‰‡"""
        try:
            # æ’é™¤æŒ‡å®šçš„ domain
            if url.startswith("https://future801113.github.io/i-love-col/"):
                print(f"   â­ï¸  è·³éæ’é™¤çš„ domain: {url[:80]}...")
                return None
            
            # æ›´å®Œæ•´çš„ headers æ¨¡ä»¿çœŸå¯¦ç€è¦½å™¨
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
                'Accept': 'image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
                'Cache-Control': 'no-cache',
                'Pragma': 'no-cache',
                'Referer': 'https://twitter.com/',
                'Sec-Ch-Ua': '"Google Chrome";v="131"',
                'Sec-Ch-Ua-Mobile': '?0',
                'Sec-Ch-Ua-Platform': '"Windows"',
                'Sec-Fetch-Dest': 'image',
                'Sec-Fetch-Mode': 'no-cors',
                'Sec-Fetch-Site': 'cross-site'
            }
            
            print(f"   ğŸ“¥ é–‹å§‹ä¸‹è¼‰: {url[:80]}...")
            print(f"   ğŸ“ ç›®æ¨™ç›®éŒ„: {self.images_dir}")
            print(f"   ğŸ“ çµ•å°è·¯å¾‘: {os.path.abspath(self.images_dir)}")
            
            # éš¨æ©Ÿå»¶é²ä¸‹è¼‰
            time.sleep(random.uniform(1, 3))
            
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            file_path = os.path.join(self.images_dir, filename)
            print(f"   ğŸ’¾ å®Œæ•´æª”æ¡ˆè·¯å¾‘: {file_path}")
            
            with open(file_path, 'wb') as f:
                f.write(response.content)
            
            # é©—è­‰æª”æ¡ˆæ˜¯å¦çœŸçš„å­˜åœ¨
            if os.path.exists(file_path):
                file_size = os.path.getsize(file_path)
                print(f"   âœ… ä¸‹è¼‰æˆåŠŸ: {filename} ({file_size} bytes)")
                return file_path
            else:
                print(f"   âŒ æª”æ¡ˆæœªæˆåŠŸå»ºç«‹: {file_path}")
                return None
            
        except Exception as e:
            print(f"   âŒ ä¸‹è¼‰å¤±æ•—: {e}")
            return None
    
    def generate_filename(self, username, index, url_hash=None):
        """ç”Ÿæˆæª”æ¡ˆåç¨±ï¼ˆåŒ…å« URL hash ç”¨æ–¼å»é‡ï¼‰ï¼Œä¸¦æª¢æŸ¥é»‘åå–®"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        if url_hash:
            filename = f"{username}_{timestamp}_{url_hash}_{index+1}.jpg"
        else:
            filename = f"{username}_{timestamp}_simple_{index+1}.jpg"
        
        # æª¢æŸ¥æ˜¯å¦åœ¨é»‘åå–®ä¸­
        if self.is_filename_blacklisted(filename):
            print(f"   ğŸš« è·³éé»‘åå–®æª”æ¡ˆ: {filename}")
            return None
            
        return filename
    
    def get_existing_image_hashes(self, username):
        """ç²å–å·²å­˜åœ¨åœ–ç‰‡çš„ URL hash é›†åˆï¼Œç”¨æ–¼å»é‡"""
        existing_hashes = set()
        
        try:
            if username == 'colne_icol':
                json_path = os.path.join(os.path.dirname(self.images_dir), 'colne_icol_images', 'images.json')
            else:
                json_path = os.path.join(os.path.dirname(self.images_dir), 'images', 'images.json')
            
            if os.path.exists(json_path):
                with open(json_path, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
                
                # å¾æª”æ¡ˆåç¨±ä¸­æå–å¯èƒ½çš„ URL hash
                for item in existing_data:
                    filename = item.get('filename', '')
                    # å¦‚æœæª”æ¡ˆåç¨±åŒ…å«å¯è­˜åˆ¥çš„ hashï¼Œæå–å®ƒ
                    if '_' in filename:
                        parts = filename.split('_')
                        if len(parts) >= 3:
                            # å‡è¨­æ ¼å¼æ˜¯ username_timestamp_hash_index.jpg
                            possible_hash = parts[2] if len(parts) > 3 else parts[-1].split('.')[0]
                            existing_hashes.add(possible_hash)
                    
                    # ä¹Ÿæª¢æŸ¥æ˜¯å¦æœ‰å„²å­˜çš„åŸå§‹ URL hash
                    if 'url_hash' in item:
                        existing_hashes.add(item['url_hash'])
            
            print(f"ğŸ“Š {username} å·²æœ‰ {len(existing_hashes)} å€‹åœ–ç‰‡ hash è¨˜éŒ„")
            return existing_hashes
            
        except Exception as e:
            print(f"âš ï¸  è®€å–å·²å­˜åœ¨åœ–ç‰‡è¨˜éŒ„æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return set()
    
    def generate_url_hash(self, url):
        """ç‚º URL ç”ŸæˆçŸ­ hashï¼Œç”¨æ–¼å»é‡å’Œæª”æ¡ˆå‘½å"""
        import hashlib
        # æ¸…ç† URLï¼Œç§»é™¤åƒæ•¸
        clean_url = url.split('?')[0].split('#')[0]
        # ç”ŸæˆçŸ­ hash
        return hashlib.md5(clean_url.encode()).hexdigest()[:8]
    
    def filter_duplicate_urls(self, image_urls, username):
        """éæ¿¾é‡è¤‡çš„åœ–ç‰‡ URL"""
        if not image_urls:
            return []
        
        existing_hashes = self.get_existing_image_hashes(username)
        unique_urls = []
        
        print(f"ğŸ” æª¢æŸ¥ {len(image_urls)} å€‹åœ–ç‰‡ URL æ˜¯å¦é‡è¤‡...")
        
        for url in image_urls:
            url_hash = self.generate_url_hash(url)
            
            if url_hash not in existing_hashes:
                unique_urls.append((url, url_hash))
                print(f"   âœ… æ–°åœ–ç‰‡: {url[:60]}... (hash: {url_hash})")
            else:
                print(f"   â­ï¸  è·³éé‡è¤‡: {url[:60]}... (hash: {url_hash})")
        
        print(f"ğŸ“Š éæ¿¾çµæœ: {len(unique_urls)} å¼µæ–°åœ–ç‰‡ / {len(image_urls)} å¼µç¸½åœ–ç‰‡")
        return unique_urls
    
    def update_images_json(self, downloaded_files, username, url_hashes=None):
        """æ›´æ–°å°æ‡‰çš„ images.json æª”æ¡ˆ"""
        try:
            if username == 'colne_icol':
                # ç‚º colne_icol å»ºç«‹ç¨ç«‹çš„ JSON æª”æ¡ˆ
                json_path = os.path.join(os.path.dirname(self.images_dir), 'colne_icol_images', 'images.json')
                relative_path = 'colne_icol_images'
            else:
                # åŸæœ¬çš„ ice_deliverer ä½¿ç”¨ç¾æœ‰çš„ JSON
                json_path = os.path.join(os.path.dirname(self.images_dir), 'images', 'images.json')
                relative_path = 'images'
            
            # è¼‰å…¥ç¾æœ‰çš„ JSON è³‡æ–™
            if os.path.exists(json_path):
                with open(json_path, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
            else:
                existing_data = []
            
            # åŠ å…¥æ–°ä¸‹è¼‰çš„åœ–ç‰‡è³‡è¨Š
            for i, file_path in enumerate(downloaded_files):
                filename = os.path.basename(file_path)
                file_size = os.path.getsize(file_path)
                
                image_info = {
                    "filename": filename,
                    "url": f"{relative_path}/{filename}",
                    "size": file_size,
                    "uploaded_at": datetime.now().isoformat()
                }
                
                # å¦‚æœæœ‰ URL hash è³‡è¨Šï¼Œä¸€ä½µå„²å­˜
                if url_hashes and i < len(url_hashes):
                    image_info["url_hash"] = url_hashes[i]
                
                existing_data.append(image_info)
            
            # å„²å­˜æ›´æ–°å¾Œçš„ JSON
            os.makedirs(os.path.dirname(json_path), exist_ok=True)
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(existing_data, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… {username} çš„åœ–ç‰‡æ¸…å–®å·²æ›´æ–° (ç¸½è¨ˆ {len(existing_data)} å¼µåœ–ç‰‡)")
            return True
            
        except Exception as e:
            print(f"âŒ æ›´æ–° {username} åœ–ç‰‡æ¸…å–®æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False
    
    def scrape_user_media(self, username, num_images=5, start_date=None, end_date=None):
        """å¾ç”¨æˆ¶çš„é é¢æŠ“å–åœ–ç‰‡"""
        if start_date or end_date:
            print(f"ğŸš€ é–‹å§‹å¾ @{username} æŠ“å–åœ–ç‰‡ (æ—¥æœŸç¯„åœ: {start_date} ~ {end_date})")
        else:
            print(f"ğŸš€ é–‹å§‹å¾ @{username} æŠ“å–åœ–ç‰‡")
        
        if not self.setup_driver():
            return []
        
        try:
            # å˜—è©¦å¤šå€‹å¯èƒ½çš„ URLï¼ˆåŒ…æ‹¬ Nitter é¡åƒï¼‰
            urls_to_try = []
            
            # å¦‚æœæŒ‡å®šäº†æ—¥æœŸï¼Œå„ªå…ˆå˜—è©¦ Nitter æœå°‹åŠŸèƒ½
            if start_date or end_date:
                print("ğŸ“… åµæ¸¬åˆ°æ—¥æœŸåƒæ•¸ï¼Œå°‡å„ªå…ˆä½¿ç”¨ Nitter æœå°‹åŠŸèƒ½")
                nitter_instances = [
                    "nitter.poast.org",
                    "nitter.it", 
                    "nitter.1d4.us",
                    "nitter.net",
                    "nitter.privacydev.net",
                    "nitter.fediverse.observer"
                ]
                
                for instance in nitter_instances:
                    search_url = self.build_nitter_search_url(username, start_date, end_date, instance)
                    urls_to_try.append(search_url)
            
            # åŠ å…¥ä¸€èˆ¬çš„ URL
            urls_to_try.extend([
                # Nitter é¡åƒç«™ï¼ˆ/media çµå°¾ï¼ŒåŠ ä¸Š f-media=on&e-nativeretweets=on åƒæ•¸ï¼‰
                f"https://nitter.poast.org/{username}/media?f-media=on&e-nativeretweets=on",
                f"https://nitter.it/{username}/media?f-media=on&e-nativeretweets=on",
                f"https://nitter.1d4.us/{username}/media?f-media=on&e-nativeretweets=on",
                f"https://nitter.net/{username}/media?f-media=on&e-nativeretweets=on",
                f"https://nitter.privacydev.net/{username}/media?f-media=on&e-nativeretweets=on",
                # å®˜æ–¹ç«™é»
                f"https://x.com/{username}",
                f"https://twitter.com/{username}",
                f"https://x.com/{username}/media",
                f"https://twitter.com/{username}/media"
            ])
            
            image_urls = []  # åˆå§‹åŒ–è®Šæ•¸
            anti_crawl_keywords = ["cloudflare", "attention required", "captcha", "access denied", "verify you are human", "too many requests"]
            
            for url in urls_to_try:
                print(f"ğŸ”— å˜—è©¦ URL: {url}")
                
                try:
                    # éš¨æ©Ÿå»¶é²ï¼ˆ2-5ç§’ï¼‰æ¨¡æ“¬çœŸå¯¦ç”¨æˆ¶
                    random_delay = random.uniform(2, 5)
                    time.sleep(random_delay)
                    
                    self.driver.get(url)
                    
                    # é é¢è¼‰å…¥å¾Œéš¨æ©Ÿç­‰å¾…ï¼ˆ8-15ç§’ï¼‰
                    random_wait = random.uniform(8, 15)
                    time.sleep(random_wait)
                    
                    # æ¨¡æ“¬æ»‘å‹•é é¢ï¼ˆçœŸå¯¦ç”¨æˆ¶è¡Œç‚ºï¼‰
                    try:
                        self.driver.execute_script("window.scrollBy(0, window.innerHeight);")
                        time.sleep(random.uniform(1, 3))
                    except:
                        pass
                    
                    current_url = self.driver.current_url
                    page_title = self.driver.title
                    print(f"   ç•¶å‰ URL: {current_url}")
                    print(f"   é é¢æ¨™é¡Œ: {page_title}")
                    page_source = self.driver.page_source.lower()
                    
                    # æª¢æŸ¥é˜²çˆ¬èŸ²å’ŒéŒ¯èª¤è¨Šæ¯
                    if any(k in page_source for k in anti_crawl_keywords):
                        print(f"   âš ï¸ é˜²çˆ¬èŸ²åµæ¸¬åˆ°é—œéµå­—ï¼Œè‡ªå‹•åˆ‡æ›ä¸‹ä¸€å€‹é¡åƒï¼")
                        continue
                    
                    # æª¢æŸ¥ HTTP éŒ¯èª¤ç¢¼ (403, 404 ç­‰) å’Œå…¶ä»–å•é¡Œ
                    if any(error in page_title for error in ["403", "404", "500", "502", "503"]):
                        print(f"   âš ï¸ æœå‹™å™¨è¿”å›éŒ¯èª¤: {page_title}ï¼Œè‡ªå‹•åˆ‡æ›ä¸‹ä¸€å€‹é¡åƒï¼")
                        continue
                    
                    if ("login" not in current_url.lower() and 
                        "signin" not in current_url.lower() and 
                        "suspended" not in page_title.lower() and
                        "error" not in page_title.lower()):
                        print("   âœ… æˆåŠŸè¼‰å…¥é é¢")
                        page_length = len(self.driver.page_source)
                        print(f"   é é¢å…§å®¹é•·åº¦: {page_length} å­—å…ƒ")
                        if page_length > 10000:
                            print("   ğŸ“· ç›´æ¥æŠ“å–åˆå§‹è¼‰å…¥çš„åœ–ç‰‡å…§å®¹")
                            image_urls = self.extract_image_urls()
                            if image_urls:
                                print(f"   åœ¨ {url} æ‰¾åˆ° {len(image_urls)} å¼µåœ–ç‰‡")
                                break
                            else:
                                print(f"   åœ¨ {url} æ²’æœ‰æ‰¾åˆ°åœ–ç‰‡ï¼Œå˜—è©¦ä¸‹ä¸€å€‹ URL")
                        else:
                            print(f"   é é¢å…§å®¹å¤ªå°‘ï¼Œå¯èƒ½è¼‰å…¥å¤±æ•—ï¼Œå˜—è©¦ä¸‹ä¸€å€‹ URL")
                    else:
                        print(f"   é é¢éœ€è¦ç™»å…¥æˆ–æœ‰å…¶ä»–å•é¡Œï¼Œå˜—è©¦ä¸‹ä¸€å€‹ URL")
                except Exception as e:
                    print(f"   è¼‰å…¥ {url} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    continue
            
            if not image_urls:
                print("ğŸ˜” æ‰€æœ‰ URL éƒ½æ²’æœ‰æ‰¾åˆ°åœ–ç‰‡")
                return []
            
            # éæ¿¾é‡è¤‡çš„åœ–ç‰‡
            unique_url_pairs = self.filter_duplicate_urls(image_urls, username)
            
            if not unique_url_pairs:
                print("ğŸ˜” æ‰€æœ‰æ‰¾åˆ°çš„åœ–ç‰‡éƒ½å·²ç¶“ä¸‹è¼‰éäº†")
                return []
            
            # å¦‚æœæ–°åœ–ç‰‡æ•¸é‡ä¸è¶³ï¼Œå˜—è©¦ç²å–æ›´å¤š
            if len(unique_url_pairs) < num_images:
                print(f"âš ï¸  æ–°åœ–ç‰‡æ•¸é‡ ({len(unique_url_pairs)}) å°‘æ–¼éœ€æ±‚ ({num_images})")
                print(f"ğŸ“¥ å°‡ä¸‹è¼‰æ‰€æœ‰ {len(unique_url_pairs)} å¼µæ–°åœ–ç‰‡")
                selected_pairs = unique_url_pairs
            else:
                # éš¨æ©Ÿé¸æ“‡è¦ä¸‹è¼‰çš„åœ–ç‰‡
                selected_pairs = random.sample(unique_url_pairs, num_images)
            
            print(f"ğŸ¯ é¸æ“‡ä¸‹è¼‰ {len(selected_pairs)} å¼µæ–°åœ–ç‰‡")
            
            # ä¸‹è¼‰åœ–ç‰‡
            downloaded_files = []
            url_hashes = []
            
            for i, (url, url_hash) in enumerate(selected_pairs):
                filename = self.generate_filename(username, i, url_hash)
                
                # å¦‚æœæª”ååœ¨é»‘åå–®ä¸­ï¼Œè·³éé€™å¼µåœ–ç‰‡
                if filename is None:
                    print(f"   â­ï¸  è·³éé»‘åå–®åœ–ç‰‡ (index {i})")
                    continue
                    
                file_path = self.download_image(url, filename)
                if file_path:
                    downloaded_files.append(file_path)
                    url_hashes.append(url_hash)
                
                # éš¨æ©Ÿå»¶é²
                time.sleep(random.uniform(1, 3))
            
            print(f"âœ… æˆåŠŸä¸‹è¼‰ {len(downloaded_files)} å¼µæ–°åœ–ç‰‡")
            
            # æ›´æ–° JSON æ™‚å‚³å…¥ URL hashes
            if downloaded_files:
                self.update_images_json(downloaded_files, username, url_hashes)
            
            return downloaded_files
            
        except Exception as e:
            print(f"âŒ æŠ“å–éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
            return []
        
        finally:
            if self.driver:
                self.driver.quit()
                print("ğŸ”’ ç€è¦½å™¨å·²é—œé–‰")
    
    def send_images_to_line_group(self, downloaded_files, username):
        """ç™¼é€å‰›ä¸‹è¼‰çš„åœ–ç‰‡åˆ° LINE ç¾¤çµ„"""
        if not self.channel_access_token or not self.target_id:
            print("âš ï¸ LINE Bot è¨­å®šä¸å®Œæ•´ï¼Œè·³é LINE ç™¼é€")
            print(f"   Channel Access Token: {'è¨­å®š' if self.channel_access_token else 'æœªè¨­å®š'}")
            print(f"   Target ID: {'è¨­å®š' if self.target_id else 'æœªè¨­å®š'}")
            return False
        
        if not downloaded_files:
            print("âš ï¸ æ²’æœ‰ä¸‹è¼‰çš„åœ–ç‰‡å¯ç™¼é€")
            return False
        
        print(f"ğŸ“± æº–å‚™ç™¼é€ {len(downloaded_files)} å¼µ {username} åœ–ç‰‡åˆ° LINE ç¾¤çµ„...")
        
        headers = {
            'Authorization': f'Bearer {self.channel_access_token}',
            'Content-Type': 'application/json'
        }
        
        messages = []
        
        # ç‚ºæ¯å¼µåœ–ç‰‡å»ºç«‹è¨Šæ¯
        for i, file_path in enumerate(downloaded_files):
            filename = os.path.basename(file_path)
            
            # å»ºç«‹ GitHub Pages URL
            if username == 'colne_icol':
                image_url = f"{self.github_pages_base}/colne_icol_images/{filename}"
            elif username == 'daily_combined':
                image_url = f"{self.github_pages_base}/combined_images/{filename}"
            else:
                image_url = f"{self.github_pages_base}/images/{filename}"
            
            print(f"ğŸ”§ DEBUG: åœ–ç‰‡ {i+1} URL: {image_url}")
            
            # æ¸¬è©¦ URL æ˜¯å¦å¯è¨ªå•
            try:
                test_response = requests.head(image_url, timeout=10)
                print(f"   ğŸ§ª URL æ¸¬è©¦: HTTP {test_response.status_code}")
                if test_response.status_code != 200:
                    print(f"   âŒ è­¦å‘Š: åœ–ç‰‡ URL ç„¡æ³•è¨ªå• ({test_response.status_code})")
            except Exception as e:
                print(f"   âŒ è­¦å‘Š: ç„¡æ³•æ¸¬è©¦ URL è¨ªå•æ€§: {e}")
            
            messages.append({
                'type': 'image',
                'originalContentUrl': image_url,
                'previewImageUrl': image_url
            })
        
        # LINE ä¸€æ¬¡æœ€å¤šç™¼é€ 5 å‰‡è¨Šæ¯
        messages = messages[:5]
        
        data = {
            'to': self.target_id,
            'messages': messages
        }
        
        try:
            print("ğŸš€ ç™¼é€ LINE API è«‹æ±‚...")
            response = requests.post(
                'https://api.line.me/v2/bot/message/push',
                headers=headers,
                json=data,
                timeout=30
            )
            
            print(f"ğŸ“¨ LINE API å›æ‡‰: {response.status_code}")
            
            if response.status_code == 200:
                print(f"âœ… æˆåŠŸç™¼é€ {len(messages)} å¼µ {username} åœ–ç‰‡åˆ° LINE ç¾¤çµ„")
                return True
            else:
                print(f"âŒ LINE ç™¼é€å¤±æ•—: {response.status_code} - {response.text}")
                return False
                
        except Exception as e:
            print(f"âŒ LINE ç™¼é€æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False

def generate_random_date_range(days_span=20):
    """ç”Ÿæˆéš¨æ©Ÿçš„æ—¥æœŸå€é–“ï¼ˆ20å¤©ï¼‰"""
    # éš¨æ©Ÿé¸æ“‡ä¸€å€‹é–‹å§‹æ—¥æœŸï¼ˆéå»ä¸€å¹´å…§ï¼‰
    today = datetime.now()
    max_days_back = 365  # æœ€å¤šå¾€å‰ä¸€å¹´
    
    # éš¨æ©Ÿé¸æ“‡é–‹å§‹æ—¥æœŸ
    random_days_back = random.randint(days_span, max_days_back)
    start_date = today - timedelta(days=random_days_back)
    end_date = start_date + timedelta(days=days_span - 1)
    
    start_str = start_date.strftime('%Y-%m-%d')
    end_str = end_date.strftime('%Y-%m-%d')
    
    return start_str, end_str

def daily_scrape_and_send():
    """æ¯æ—¥æŠ“åœ–ã€æäº¤æ¨é€ã€ç™¼é€åˆ° LINE ç¾¤çµ„ - GitHub Actions ç‰ˆæœ¬
    
    ä¸‰å±¤å›é€€ç­–ç•¥:
    1. å…ˆæŠ“æœ€è¿‘ä¸‰å¤©çš„åœ–ç‰‡
    2. å¦‚æœæ²’æœ‰æ–°åœ–ï¼Œç”¨åŸæœ¬çš„éš¨æ©Ÿé‚è¼¯ (éš¨æ©Ÿ20å¤©å€é–“)
    3. å¦‚æœé‚„æ˜¯æ²’æœ‰ï¼Œä½¿ç”¨ combined_images è£¡é¢éš¨æ©ŸæŠ“ä¸€å¼µ
    """
    print("ğŸ¤– é–‹å§‹æ¯æ—¥æŠ“åœ–ã€æäº¤æ¨é€ã€ç™¼é€åˆ° LINE ç¾¤çµ„...")
    print("=" * 50)
    
    accounts = ['ice_deliverer', 'colne_icol']
    num_images_per_account = 2  # æ¯å€‹å¸³è™ŸæŠ“ 2 å¼µç”¨æ–¼çµ„åˆ
    
    all_downloaded_files = {}  # å„²å­˜æ‰€æœ‰ä¸‹è¼‰çš„æª”æ¡ˆ
    
    # å…§éƒ¨å‡½æ•¸ï¼šä½¿ç”¨æŒ‡å®šæ—¥æœŸç¯„åœå˜—è©¦æŠ“åœ–
    def run_scrape_for_dates(start_date, end_date, strategy_name):
        """ä½¿ç”¨æŒ‡å®šæ—¥æœŸç¯„åœæŠ“åœ–"""
        print(f"\nğŸ”„ {strategy_name}")
        temp_files = {}
        
        for account in accounts:
            print(f"   ğŸ“· {account}...", end=" ")
            scraper = TwitterImageScraperSimple(username=account)
            downloaded_files = scraper.scrape_user_media(account, num_images_per_account, start_date, end_date)
            
            if downloaded_files:
                print(f"âœ… {len(downloaded_files)} å¼µ")
                temp_files[account] = downloaded_files
            else:
                print("ğŸ˜” ç„¡æ–°åœ–")
                temp_files[account] = []
            
            # å…©å€‹å¸³è™Ÿä¹‹é–“ç­‰å¾…
            if account != accounts[-1]:
                time.sleep(2)
        
        return temp_files
    
    # æ¸¬è©¦ hook: å¼·åˆ¶æ¨¡æ“¬æ²’æœ‰æ–°åœ–çš„æƒ…æ³
    if os.environ.get('FORCE_NO_SCRAPE', '').lower() == 'true':
        print("\nğŸ§ª [TEST MODE] FORCE_NO_SCRAPE=true - æ¨¡æ“¬æ²’æœ‰ä¸‹è¼‰åˆ°æ–°åœ–ç‰‡")
        all_downloaded_files = {account: [] for account in accounts}
        total_new_images = 0
        # ç›´æ¥è·³åˆ°ç¬¬3å€‹ç­–ç•¥ï¼ˆå‚™ç”¨åœ–ç‰‡ï¼‰
        print("\nğŸ¯ ç­–ç•¥ 3/3: ä»ç„¡æ–°åœ–ï¼Œä½¿ç”¨ combined_images å‚™ç”¨")
    else:
        # ç­–ç•¥ 1: å…ˆæŠ“æœ€è¿‘ä¸‰å¤©
        print("\nğŸ¯ ç­–ç•¥ 1/3: æŠ“æœ€è¿‘ä¸‰å¤©çš„åœ–ç‰‡")
        today = datetime.now()
        three_days_ago = today - timedelta(days=3)
        start_date = three_days_ago.strftime('%Y-%m-%d')
        end_date = today.strftime('%Y-%m-%d')
        print(f"   æ—¥æœŸç¯„åœ: {start_date} ~ {end_date}")
        
        all_downloaded_files = run_scrape_for_dates(start_date, end_date, "å˜—è©¦æŠ“æœ€è¿‘ä¸‰å¤©")
        total_new_images = sum(len(files) for files in all_downloaded_files.values())
        
        # ç­–ç•¥ 2: å¦‚æœæ²’æœ‰æ–°åœ–ï¼Œç”¨éš¨æ©Ÿé‚è¼¯
        if total_new_images == 0:
            print("\nğŸ¯ ç­–ç•¥ 2/3: æ²’æœ‰æœ€è¿‘æ–°åœ–ï¼Œæ”¹ç”¨éš¨æ©Ÿ20å¤©å€é–“")
            random_start, random_end = generate_random_date_range()
            print(f"   æ—¥æœŸç¯„åœ: {random_start} ~ {random_end}")
            
            all_downloaded_files = run_scrape_for_dates(random_start, random_end, "å˜—è©¦æŠ“éš¨æ©Ÿæ—¥æœŸç¯„åœ")
            total_new_images = sum(len(files) for files in all_downloaded_files.values())
        
        # ç­–ç•¥ 3: å¦‚æœé‚„æ˜¯æ²’æœ‰æ–°åœ–ï¼Œä½¿ç”¨ combined_images å‚™ç”¨
        if total_new_images == 0:
            print("\nğŸ¯ ç­–ç•¥ 3/3: ä»ç„¡æ–°åœ–ï¼Œä½¿ç”¨ combined_images å‚™ç”¨")
    
    
    # æ­¥é©Ÿ 2: å»ºç«‹çµ„åˆåœ–ç‰‡æˆ–é¸æ“‡å‚™ç”¨åœ–ç‰‡
    print(f"\nğŸ¨ æ­¥é©Ÿ 2/3: å»ºç«‹çµ„åˆåœ–ç‰‡æˆ–é¸æ“‡å‚™ç”¨")
    combined_image_path = None
    backup_image_path = None
    
    if total_new_images > 0:
        # æœ‰æ–°åœ–ç‰‡ï¼Œå»ºç«‹çµ„åˆåœ–ç‰‡
        try:
            from image_combiner import create_combined_from_new_images
            combined_image_path = create_combined_from_new_images(all_downloaded_files)
            
            if combined_image_path:
                print("   âœ… çµ„åˆåœ–ç‰‡å»ºç«‹æˆåŠŸ")
            else:
                print("   âŒ çµ„åˆåœ–ç‰‡å»ºç«‹å¤±æ•—")
                
        except ImportError:
            print("   âš ï¸ çµ„åˆåœ–ç‰‡æ¨¡çµ„ä¸å­˜åœ¨ï¼Œè·³éçµ„åˆåœ–ç‰‡å»ºç«‹")
        except Exception as e:
            print(f"   âŒ å»ºç«‹çµ„åˆåœ–ç‰‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    else:
        # æ²’æœ‰æ–°åœ–ç‰‡ï¼Œå¾ combined_images éš¨æ©Ÿé¸æ“‡å‚™ç”¨åœ–ç‰‡ï¼ˆæ’é™¤æœ€æ–°çš„ï¼‰
        print("   ğŸ˜” æ²’æœ‰æ–°åœ–ç‰‡ï¼Œå¾ combined_images å‚™ç”¨åº«éš¨æ©Ÿé¸æ“‡...")
        combined_dir = os.path.normpath(os.path.join(os.path.dirname(__file__), '..', 'combined_images'))
        
        if os.path.exists(combined_dir):
            try:
                image_files = [f for f in os.listdir(combined_dir) 
                              if f.lower().endswith(('.jpg', '.jpeg', '.png', '.gif'))]
                
                if len(image_files) > 1:
                    # æŒ‰ä¿®æ”¹æ™‚é–“æ’åºï¼Œæ’é™¤æœ€æ–°çš„
                    image_files_with_time = [(f, os.path.getmtime(os.path.join(combined_dir, f))) 
                                            for f in image_files]
                    image_files_with_time.sort(key=lambda x: x[1], reverse=True)
                    
                    # æ’é™¤æœ€æ–°çš„ï¼Œå¾å‰©é¤˜çš„éš¨æ©Ÿé¸æ“‡
                    eligible_images = image_files_with_time[1:]  # è·³éæœ€æ–°çš„
                    
                    if eligible_images:
                        backup_image = random.choice(eligible_images)[0]
                        backup_image_path = os.path.join(combined_dir, backup_image)
                        print(f"   ğŸ“¸ å¾çµ„åˆåœ–ç‰‡å‚™ç”¨åº«éš¨æ©Ÿé¸æ“‡ï¼ˆæ’é™¤æœ€æ–°ï¼‰: {backup_image}")
                    else:
                        print(f"   âš ï¸ æ²’æœ‰ç¬¦åˆæ¢ä»¶çš„å‚™ç”¨åœ–ç‰‡ï¼ˆåªæœ‰æœ€æ–°çš„ï¼‰")
                        
                elif len(image_files) == 1:
                    # åªæœ‰ä¸€å¼µï¼Œç›´æ¥ä½¿ç”¨
                    backup_image = image_files[0]
                    backup_image_path = os.path.join(combined_dir, backup_image)
                    print(f"   ğŸ“¸ å‚™ç”¨åº«åªæœ‰ä¸€å¼µåœ–ç‰‡ï¼Œç›´æ¥ä½¿ç”¨: {backup_image}")
                else:
                    print(f"   âš ï¸ combined_images ç›®éŒ„ç‚ºç©º")
                    
            except Exception as e:
                print(f"   âš ï¸ é¸æ“‡å‚™ç”¨åœ–ç‰‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        else:
            print(f"   âš ï¸ combined_images ç›®éŒ„ä¸å­˜åœ¨: {combined_dir}")
    
    # æ­¥é©Ÿ 3: ç™¼é€çµ„åˆåœ–ç‰‡åˆ° LINE ç¾¤çµ„ (å¦‚æœæœªè¢«ç¦ç”¨)
    if os.environ.get('SKIP_LINE_SEND', '').lower() == 'true':
        print(f"\nğŸ“± è·³é LINE ç™¼é€ (SKIP_LINE_SEND=true)")
        print(f"\nğŸ‰ æ¯æ—¥ä»»å‹™å®Œæˆï¼")
        print(f"ğŸ“Š ç¸½è¨ˆä¸‹è¼‰ {total_new_images} å¼µåœ–ç‰‡")
        if combined_image_path:
            print(f"ğŸ–¼ï¸ å·²å»ºç«‹çµ„åˆåœ–ç‰‡: {combined_image_path}")
        elif backup_image_path:
            print(f"ğŸ–¼ï¸ å·²é¸æ“‡å‚™ç”¨åœ–ç‰‡: {backup_image_path}")
        return True
    
    print(f"\nğŸ“± æ­¥é©Ÿ 3/3: ç™¼é€åœ–ç‰‡åˆ° LINE ç¾¤çµ„")
    
    total_sent = 0
    image_to_send = combined_image_path or backup_image_path
    
    if image_to_send:
        try:
            # ä½¿ç”¨ç¬¬ä¸€å€‹å¸³è™Ÿçš„ scraper ä¾†ç™¼é€åœ–ç‰‡
            scraper = TwitterImageScraperSimple(username=accounts[0])
            
            # æ±ºå®šç™¼é€çš„é¡å‹
            send_type = "daily_combined" if combined_image_path else "daily_backup"
            
            # ç™¼é€åœ–ç‰‡
            if scraper.send_images_to_line_group([image_to_send], send_type):
                if combined_image_path:
                    print("   âœ… çµ„åˆåœ–ç‰‡å·²ç™¼é€åˆ° LINE ç¾¤çµ„")
                else:
                    print("   âœ… å‚™ç”¨åœ–ç‰‡å·²ç™¼é€åˆ° LINE ç¾¤çµ„")
                total_sent = 1
            else:
                print("   âŒ åœ–ç‰‡ç™¼é€å¤±æ•—")
                total_sent = 0
                
        except Exception as e:
            print(f"   âŒ ç™¼é€åœ–ç‰‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            total_sent = 0
    else:
        print("   ğŸ˜” æ²’æœ‰å¯ç™¼é€çš„åœ–ç‰‡ï¼ˆç„¡æ–°åœ–ç‰‡ï¼Œä¹Ÿç„¡å‚™ç”¨åœ–ç‰‡ï¼‰")

    print(f"\nğŸ‰ æ¯æ—¥ä»»å‹™å®Œæˆï¼")
    print(f"ğŸ“Š ç¸½è¨ˆä¸‹è¼‰ {total_new_images} å¼µåœ–ç‰‡")
    print(f"ğŸ“Š ç¸½è¨ˆç™¼é€ {total_sent} å¼µåœ–ç‰‡åˆ° LINE ç¾¤çµ„")
    if combined_image_path:
        print(f"ğŸ–¼ï¸ é¡å¤–å»ºç«‹äº† 1 å¼µæ–°çµ„åˆåœ–ç‰‡")
    elif backup_image_path:
        print(f"ğŸ–¼ï¸ ç™¼é€äº† 1 å¼µå‚™ç”¨çµ„åˆåœ–ç‰‡ï¼ˆå› ç‚ºä»Šæ—¥ç„¡æ–°åœ–ç‰‡ï¼‰")
    
    return total_sent > 0

def main():
    """ä¸»å‡½æ•¸"""
    if len(sys.argv) < 2 or sys.argv[1] in ['--help', '-h', 'help']:
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  python web_scraper_simple.py <username> [num_images] [start_date] [end_date]")
        print("  python web_scraper_simple.py daily  # æ¯æ—¥æ¨¡å¼ï¼šå…©å€‹å¸³è™Ÿå„æŠ“2å¼µä¸¦ç™¼é€åˆ°LINE")
        print("\nç¯„ä¾‹:")
        print("  python web_scraper_simple.py ice_deliverer 5")
        print("  python web_scraper_simple.py ice_deliverer 5 2024-01-01 2024-01-31")
        print("  python web_scraper_simple.py ice_deliverer 5 random  (ä½¿ç”¨éš¨æ©Ÿ10å¤©å€é–“)")
        print("  python web_scraper_simple.py daily  (æ¯æ—¥è‡ªå‹•æŠ“åœ–ä¸¦ç™¼LINE)")
        print("\næ”¯æ´çš„å¸³è™Ÿï¼š")
        print("  - ice_deliverer")
        print("  - colne_icol")
        return
    
    # æª¢æŸ¥æ˜¯å¦ç‚ºæ¯æ—¥æ¨¡å¼
    if sys.argv[1].lower() == 'daily':
        daily_scrape_and_send()
        return
    
    username = sys.argv[1]
    
    # é©—è­‰ä½¿ç”¨è€…åç¨±
    valid_usernames = ['ice_deliverer', 'colne_icol']
    if username not in valid_usernames:
        print(f"âŒ ä¸æ”¯æ´çš„ä½¿ç”¨è€…åç¨±: {username}")
        print(f"âœ… æ”¯æ´çš„ä½¿ç”¨è€…åç¨±: {', '.join(valid_usernames)}")
        return
    
    num_images = int(sys.argv[2]) if len(sys.argv) > 2 else 5
    
    # æª¢æŸ¥æ˜¯å¦ä½¿ç”¨éš¨æ©Ÿæ—¥æœŸ
    if len(sys.argv) > 3 and sys.argv[3].lower() == 'random':
        start_date, end_date = generate_random_date_range()
        print(f"ğŸ² ä½¿ç”¨éš¨æ©Ÿæ—¥æœŸå€é–“: {start_date} ~ {end_date}")
    else:
        start_date = sys.argv[3] if len(sys.argv) > 3 else None
        end_date = sys.argv[4] if len(sys.argv) > 4 else None
        # å¦‚æœæ²’æœ‰æŒ‡å®šæ—¥æœŸï¼Œé»˜èªä½¿ç”¨éš¨æ©Ÿæ—¥æœŸ
        if not start_date and not end_date:
            start_date, end_date = generate_random_date_range()
            print(f"ğŸ² é»˜èªä½¿ç”¨éš¨æ©Ÿæ—¥æœŸå€é–“: {start_date} ~ {end_date}")
    
    scraper = TwitterImageScraperSimple(username=username)
    downloaded_files = scraper.scrape_user_media(username, num_images, start_date, end_date)
    
    if downloaded_files:
        print(f"\nğŸ‰ æŠ“å–å®Œæˆï¼ä¸‹è¼‰äº† {len(downloaded_files)} å¼µåœ–ç‰‡:")
        for file_path in downloaded_files:
            print(f"   ğŸ“¸ {os.path.basename(file_path)}")
        
        # æ›´æ–°å°æ‡‰çš„åœ–ç‰‡æ¸…å–®
        print(f"\nğŸ“ åœ–ç‰‡æ¸…å–®å·²åœ¨ä¸‹è¼‰éç¨‹ä¸­è‡ªå‹•æ›´æ–°")
        
    else:
        print("\nğŸ˜” æ²’æœ‰ä¸‹è¼‰ä»»ä½•åœ–ç‰‡")

if __name__ == '__main__':
    main()
