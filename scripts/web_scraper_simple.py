#!/usr/bin/env python3
"""
ç°¡åŒ–ç‰ˆ Twitter åœ–ç‰‡çˆ¬èŸ² - GitHub Actions ç‰ˆæœ¬
ä½¿ç”¨å…¬é–‹çš„ç”¨æˆ¶é é¢è€Œä¸æ˜¯æœå°‹é é¢
"""

import os
import sys
import json
import time
import random
import requests
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from selenium.webdriver.common.keys import Keys

class TwitterImageScraperSimple:
    def __init__(self, config_file='config.json', username=None):
        self.config_file = config_file
        self.config = self.load_config()
        self.username = username
        self.images_dir = self.get_images_directory(username)
        self.setup_directories()
        self.driver = None
        
        # LINE Bot è¨­å®š - æ”¯æ´ç’°å¢ƒè®Šæ•¸
        self.line_config = self.config.get('line_bot', {})
        self.channel_access_token = (
            os.environ.get('LINE_CHANNEL_ACCESS_TOKEN') or 
            self.line_config.get('channel_access_token', '')
        )
        self.target_id = (
            os.environ.get('LINE_TARGET_ID') or 
            self.line_config.get('target_id', '')
        )
        self.github_pages_base = self.line_config.get('github_pages_base', 'https://future801113.github.io/i-love-col')
    
    def get_images_directory(self, username):
        """æ ¹æ“šä½¿ç”¨è€…åç¨±æ±ºå®šåœ–ç‰‡å­˜æ”¾ç›®éŒ„ - GitHub Actions ç‰ˆæœ¬"""
        if username == 'colne_icol':
            return self.config.get('colne_icol_directory', '../colne_icol_images')
        else:
            return self.config.get('images_directory', '../images')
        
    def load_config(self):
        """è¼‰å…¥è¨­å®šæª”"""
        try:
            with open(self.config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"âŒ æ‰¾ä¸åˆ°è¨­å®šæª”: {self.config_file}")
            # åœ¨ GitHub Actions ç’°å¢ƒä¸­ï¼Œæä¾›é»˜èªè¨­å®š
            return {
                "images_directory": "../images",
                "colne_icol_directory": "../colne_icol_images",
                "line_bot": {
                    "github_pages_base": "https://future801113.github.io/i-love-col"
                }
            }
        except json.JSONDecodeError as e:
            print(f"âŒ è¨­å®šæª”æ ¼å¼éŒ¯èª¤: {e}")
            return {}
    
    def setup_directories(self):
        """å»ºç«‹å¿…è¦çš„ç›®éŒ„"""
        if not os.path.exists(self.images_dir):
            os.makedirs(self.images_dir)
            print(f"ğŸ“ å»ºç«‹åœ–ç‰‡ç›®éŒ„: {self.images_dir}")
    
    def load_blacklist(self):
        """è¼‰å…¥é»‘åå–®æª”æ¡ˆ"""
        try:
            blacklist_path = os.path.join(os.path.dirname(self.images_dir), 'images', 'blacklist.json')
            if os.path.exists(blacklist_path):
                with open(blacklist_path, 'r', encoding='utf-8') as f:
                    blacklist_data = json.load(f)
                    return {item['filename'] for item in blacklist_data}
            return set()
        except Exception as e:
            print(f"âš ï¸  è¼‰å…¥é»‘åå–®æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return set()
    
    def is_filename_blacklisted(self, filename):
        """æª¢æŸ¥æª”åæ˜¯å¦åœ¨é»‘åå–®ä¸­"""
        blacklist = self.load_blacklist()
        return filename in blacklist
    
    def setup_driver(self):
        """è¨­å®š Chrome ç€è¦½å™¨é©…å‹• - GitHub Actions ç‰ˆæœ¬"""
        chrome_options = Options()
        # GitHub Actions å¿…éœ€çš„é¸é …
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--disable-software-rasterizer')
        chrome_options.add_argument('--window-size=1200,800')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36')
        
        try:
            self.driver = webdriver.Chrome(options=chrome_options)
            # éš±è— webdriver ç‰¹å¾µ
            self.driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
                'source': '''
                    Object.defineProperty(navigator, 'webdriver', {
                        get: () => undefined
                    })
                '''
            })
            print("âœ… Chrome ç€è¦½å™¨é©…å‹•åˆå§‹åŒ–æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ Chrome ç€è¦½å™¨é©…å‹•åˆå§‹åŒ–å¤±æ•—: {e}")
            return False
    
    def build_nitter_search_url(self, username, start_date=None, end_date=None, nitter_instance="nitter.net"):
        """å»ºç«‹ Nitter æœå°‹ URLï¼ˆæ”¯æ´æ—¥æœŸå€é–“ï¼‰"""
        import urllib.parse
        # å»ºç«‹æœå°‹æŸ¥è©¢
        if start_date and end_date:
            query = f"from:{username} since:{start_date} until:{end_date}"
        elif start_date:
            query = f"from:{username} since:{start_date}"
        else:
            query = f"from:{username}"
        encoded_query = urllib.parse.quote(query)
        # åŠ ä¸Š f-media=on & e-nativeretweets=on åƒæ•¸
        return f"https://{nitter_instance}/search?q={encoded_query}&f=tweets&f-media=on&e-nativeretweets=on"
    
    def extract_image_urls(self):
        """æå–åœ–ç‰‡ URL"""
        image_urls = []
        
        try:
            print("ğŸ” é–‹å§‹æå–åœ–ç‰‡ URL...")
            
            # åˆ¤æ–·æ˜¯å¦ç‚º Nitter é é¢
            current_url = self.driver.current_url.lower()
            is_nitter = "nitter" in current_url
            
            if is_nitter:
                print("ğŸ¦ æª¢æ¸¬åˆ° Nitter é é¢ï¼Œä½¿ç”¨ Nitter é¸æ“‡å™¨")
                # Nitter å°ˆç”¨é¸æ“‡å™¨
                selectors = [
                    ".still-image img",
                    ".attachment img",
                    ".gallery-row img",
                    "a.still-image",
                ]
            else:
                print("ğŸ”µ ä½¿ç”¨ Twitter/X å®˜æ–¹é¸æ“‡å™¨")
                # Twitter/X å®˜æ–¹é¸æ“‡å™¨
                selectors = [
                    "img[src*='pbs.twimg.com'][src*='media']",
                    "div[data-testid='tweetPhoto'] img",
                    "[data-testid='tweetPhoto'] img",
                    "article img[src*='pbs.twimg.com']",
                    "img[alt*='Image']"
                ]
            
            for selector in selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    print(f"   ä½¿ç”¨é¸æ“‡å™¨ '{selector}' æ‰¾åˆ° {len(elements)} å€‹å…ƒç´ ")
                    
                    for element in elements:
                        try:
                            # å°æ–¼ Nitter çš„ a.still-image é€£çµï¼Œä½¿ç”¨ href å±¬æ€§
                            if is_nitter and selector == "a.still-image":
                                href = element.get_attribute("href")
                                if href and "/pic/orig/" in href:
                                    if "media%2F" in href:
                                        media_id = href.split("media%2F")[1].split(".")[0]
                                        large_url = f"https://pbs.twimg.com/media/{media_id}?format=jpg&name=large"
                                        if large_url not in image_urls:
                                            image_urls.append(large_url)
                                            print(f"   âœ… æ‰¾åˆ°åœ–ç‰‡ (from href): {large_url[:80]}...")
                            else:
                                # ä¸€èˆ¬çš„ img å…ƒç´ 
                                src = element.get_attribute("src")
                                if src:
                                    if is_nitter:
                                        if "/pic/media%2F" in src:
                                            media_id = src.split("media%2F")[1].split(".")[0].split("%")[0]
                                            large_url = f"https://pbs.twimg.com/media/{media_id}?format=jpg&name=large"
                                            if large_url not in image_urls:
                                                image_urls.append(large_url)
                                                print(f"   âœ… æ‰¾åˆ°åœ–ç‰‡ (from src): {large_url[:80]}...")
                                    else:
                                        # Twitter/X å®˜æ–¹è™•ç†
                                        if ("pbs.twimg.com" in src or "pic.twitter.com" in src):
                                            if "?format=" in src or "&name=" in src:
                                                base_url = src.split("?")[0]
                                                large_url = f"{base_url}?format=jpg&name=large"
                                            else:
                                                large_url = src
                                            
                                            if large_url not in image_urls:
                                                image_urls.append(large_url)
                                                print(f"   âœ… æ‰¾åˆ°åœ–ç‰‡: {large_url[:80]}...")
                        except Exception as e:
                            continue
                except Exception as e:
                    print(f"   é¸æ“‡å™¨åŸ·è¡Œå¤±æ•—: {e}")
                    continue
            
            # å»é‡ä¸¦é™åˆ¶æ•¸é‡
            unique_urls = list(dict.fromkeys(image_urls))
            print(f"ğŸ–¼ï¸  ç¸½å…±æ‰¾åˆ° {len(unique_urls)} å¼µå”¯ä¸€åœ–ç‰‡")
            
            return unique_urls
            
        except Exception as e:
            print(f"âŒ æå–åœ–ç‰‡ URL æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return []
    
    def download_image(self, url, filename):
        """ä¸‹è¼‰åœ–ç‰‡"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
                'Referer': 'https://x.com/'
            }
            
            print(f"   ğŸ“¥ é–‹å§‹ä¸‹è¼‰: {url[:80]}...")
            print(f"   ğŸ“ ç›®æ¨™ç›®éŒ„: {self.images_dir}")
            print(f"   ğŸ“ çµ•å°è·¯å¾‘: {os.path.abspath(self.images_dir)}")
            
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            file_path = os.path.join(self.images_dir, filename)
            print(f"   ğŸ’¾ å®Œæ•´æª”æ¡ˆè·¯å¾‘: {file_path}")
            
            with open(file_path, 'wb') as f:
                f.write(response.content)
            
            # é©—è­‰æª”æ¡ˆæ˜¯å¦çœŸçš„å­˜åœ¨
            if os.path.exists(file_path):
                file_size = os.path.getsize(file_path)
                print(f"   âœ… ä¸‹è¼‰æˆåŠŸ: {filename} ({file_size} bytes)")
                return file_path
            else:
                print(f"   âŒ æª”æ¡ˆæœªæˆåŠŸå»ºç«‹: {file_path}")
                return None
            
        except Exception as e:
            print(f"   âŒ ä¸‹è¼‰å¤±æ•—: {e}")
            return None
    
    def generate_filename(self, username, index, url_hash=None):
        """ç”Ÿæˆæª”æ¡ˆåç¨±ï¼ˆåŒ…å« URL hash ç”¨æ–¼å»é‡ï¼‰ï¼Œä¸¦æª¢æŸ¥é»‘åå–®"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        if url_hash:
            filename = f"{username}_{timestamp}_{url_hash}_{index+1}.jpg"
        else:
            filename = f"{username}_{timestamp}_simple_{index+1}.jpg"
        
        # æª¢æŸ¥æ˜¯å¦åœ¨é»‘åå–®ä¸­
        if self.is_filename_blacklisted(filename):
            print(f"   ğŸš« è·³éé»‘åå–®æª”æ¡ˆ: {filename}")
            return None
            
        return filename
    
    def get_existing_image_hashes(self, username):
        """ç²å–å·²å­˜åœ¨åœ–ç‰‡çš„ URL hash é›†åˆï¼Œç”¨æ–¼å»é‡"""
        existing_hashes = set()
        
        try:
            if username == 'colne_icol':
                json_path = os.path.join(os.path.dirname(self.images_dir), 'colne_icol_images', 'images.json')
            else:
                json_path = os.path.join(os.path.dirname(self.images_dir), 'images', 'images.json')
            
            if os.path.exists(json_path):
                with open(json_path, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
                
                # å¾æª”æ¡ˆåç¨±ä¸­æå–å¯èƒ½çš„ URL hash
                for item in existing_data:
                    filename = item.get('filename', '')
                    # å¦‚æœæª”æ¡ˆåç¨±åŒ…å«å¯è­˜åˆ¥çš„ hashï¼Œæå–å®ƒ
                    if '_' in filename:
                        parts = filename.split('_')
                        if len(parts) >= 3:
                            # å‡è¨­æ ¼å¼æ˜¯ username_timestamp_hash_index.jpg
                            possible_hash = parts[2] if len(parts) > 3 else parts[-1].split('.')[0]
                            existing_hashes.add(possible_hash)
                    
                    # ä¹Ÿæª¢æŸ¥æ˜¯å¦æœ‰å„²å­˜çš„åŸå§‹ URL hash
                    if 'url_hash' in item:
                        existing_hashes.add(item['url_hash'])
            
            print(f"ğŸ“Š {username} å·²æœ‰ {len(existing_hashes)} å€‹åœ–ç‰‡ hash è¨˜éŒ„")
            return existing_hashes
            
        except Exception as e:
            print(f"âš ï¸  è®€å–å·²å­˜åœ¨åœ–ç‰‡è¨˜éŒ„æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return set()
    
    def generate_url_hash(self, url):
        """ç‚º URL ç”ŸæˆçŸ­ hashï¼Œç”¨æ–¼å»é‡å’Œæª”æ¡ˆå‘½å"""
        import hashlib
        # æ¸…ç† URLï¼Œç§»é™¤åƒæ•¸
        clean_url = url.split('?')[0].split('#')[0]
        # ç”ŸæˆçŸ­ hash
        return hashlib.md5(clean_url.encode()).hexdigest()[:8]
    
    def filter_duplicate_urls(self, image_urls, username):
        """éæ¿¾é‡è¤‡çš„åœ–ç‰‡ URL"""
        if not image_urls:
            return []
        
        existing_hashes = self.get_existing_image_hashes(username)
        unique_urls = []
        
        print(f"ğŸ” æª¢æŸ¥ {len(image_urls)} å€‹åœ–ç‰‡ URL æ˜¯å¦é‡è¤‡...")
        
        for url in image_urls:
            url_hash = self.generate_url_hash(url)
            
            if url_hash not in existing_hashes:
                unique_urls.append((url, url_hash))
                print(f"   âœ… æ–°åœ–ç‰‡: {url[:60]}... (hash: {url_hash})")
            else:
                print(f"   â­ï¸  è·³éé‡è¤‡: {url[:60]}... (hash: {url_hash})")
        
        print(f"ğŸ“Š éæ¿¾çµæœ: {len(unique_urls)} å¼µæ–°åœ–ç‰‡ / {len(image_urls)} å¼µç¸½åœ–ç‰‡")
        return unique_urls
    
    def update_images_json(self, downloaded_files, username, url_hashes=None):
        """æ›´æ–°å°æ‡‰çš„ images.json æª”æ¡ˆ"""
        try:
            if username == 'colne_icol':
                # ç‚º colne_icol å»ºç«‹ç¨ç«‹çš„ JSON æª”æ¡ˆ
                json_path = os.path.join(os.path.dirname(self.images_dir), 'colne_icol_images', 'images.json')
                relative_path = 'colne_icol_images'
            else:
                # åŸæœ¬çš„ ice_deliverer ä½¿ç”¨ç¾æœ‰çš„ JSON
                json_path = os.path.join(os.path.dirname(self.images_dir), 'images', 'images.json')
                relative_path = 'images'
            
            # è¼‰å…¥ç¾æœ‰çš„ JSON è³‡æ–™
            if os.path.exists(json_path):
                with open(json_path, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
            else:
                existing_data = []
            
            # åŠ å…¥æ–°ä¸‹è¼‰çš„åœ–ç‰‡è³‡è¨Š
            for i, file_path in enumerate(downloaded_files):
                filename = os.path.basename(file_path)
                file_size = os.path.getsize(file_path)
                
                image_info = {
                    "filename": filename,
                    "url": f"{relative_path}/{filename}",
                    "size": file_size,
                    "uploaded_at": datetime.now().isoformat()
                }
                
                # å¦‚æœæœ‰ URL hash è³‡è¨Šï¼Œä¸€ä½µå„²å­˜
                if url_hashes and i < len(url_hashes):
                    image_info["url_hash"] = url_hashes[i]
                
                existing_data.append(image_info)
            
            # å„²å­˜æ›´æ–°å¾Œçš„ JSON
            os.makedirs(os.path.dirname(json_path), exist_ok=True)
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(existing_data, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… {username} çš„åœ–ç‰‡æ¸…å–®å·²æ›´æ–° (ç¸½è¨ˆ {len(existing_data)} å¼µåœ–ç‰‡)")
            return True
            
        except Exception as e:
            print(f"âŒ æ›´æ–° {username} åœ–ç‰‡æ¸…å–®æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False
    
    def scrape_user_media(self, username, num_images=5, start_date=None, end_date=None):
        """å¾ç”¨æˆ¶çš„é é¢æŠ“å–åœ–ç‰‡"""
        if start_date or end_date:
            print(f"ğŸš€ é–‹å§‹å¾ @{username} æŠ“å–åœ–ç‰‡ (æ—¥æœŸç¯„åœ: {start_date} ~ {end_date})")
        else:
            print(f"ğŸš€ é–‹å§‹å¾ @{username} æŠ“å–åœ–ç‰‡")
        
        if not self.setup_driver():
            return []
        
        try:
            # å˜—è©¦å¤šå€‹å¯èƒ½çš„ URLï¼ˆåŒ…æ‹¬ Nitter é¡åƒï¼‰
            urls_to_try = []
            
            # å¦‚æœæŒ‡å®šäº†æ—¥æœŸï¼Œå„ªå…ˆå˜—è©¦ Nitter æœå°‹åŠŸèƒ½
            if start_date or end_date:
                print("ğŸ“… åµæ¸¬åˆ°æ—¥æœŸåƒæ•¸ï¼Œå°‡å„ªå…ˆä½¿ç”¨ Nitter æœå°‹åŠŸèƒ½")
                nitter_instances = [
                    "nitter.net",
                    "nitter.it", 
                    "nitter.1d4.us",
                    "nitter.poast.org",
                    "nitter.privacydev.net"
                ]
                
                for instance in nitter_instances:
                    search_url = self.build_nitter_search_url(username, start_date, end_date, instance)
                    urls_to_try.append(search_url)
            
            # åŠ å…¥ä¸€èˆ¬çš„ URL
            urls_to_try.extend([
                # Nitter é¡åƒç«™ï¼ˆé€šå¸¸ä¸éœ€è¦ç™»å…¥ï¼‰ï¼ŒåŠ ä¸Š f-media=on&e-nativeretweets=on åƒæ•¸
                f"https://nitter.net/{username}?f-media=on&e-nativeretweets=on",
                f"https://nitter.it/{username}?f-media=on&e-nativeretweets=on",
                f"https://nitter.1d4.us/{username}?f-media=on&e-nativeretweets=on",
                f"https://nitter.poast.org/{username}?f-media=on&e-nativeretweets=on",
                f"https://nitter.privacydev.net/{username}?f-media=on&e-nativeretweets=on",
                # å®˜æ–¹ç«™é»
                f"https://x.com/{username}",
                f"https://twitter.com/{username}",
                f"https://x.com/{username}/media",
                f"https://twitter.com/{username}/media"
            ])
            
            image_urls = []  # åˆå§‹åŒ–è®Šæ•¸
            
            for url in urls_to_try:
                print(f"ğŸ”— å˜—è©¦ URL: {url}")
                
                try:
                    self.driver.get(url)
                    time.sleep(8)
                    
                    # æª¢æŸ¥é é¢ç‹€æ…‹
                    current_url = self.driver.current_url
                    page_title = self.driver.title
                    
                    print(f"   ç•¶å‰ URL: {current_url}")
                    print(f"   é é¢æ¨™é¡Œ: {page_title}")
                    
                    # æª¢æŸ¥æ˜¯å¦æˆåŠŸè¼‰å…¥ï¼ˆä¸æ˜¯ç™»å…¥é é¢æˆ–éŒ¯èª¤é é¢ï¼‰
                    if ("login" not in current_url.lower() and 
                        "signin" not in current_url.lower() and 
                        "suspended" not in page_title.lower() and
                        "error" not in page_title.lower()):
                        
                        print("   âœ… æˆåŠŸè¼‰å…¥é é¢")
                        
                        # æª¢æŸ¥é é¢å…§å®¹é•·åº¦
                        page_length = len(self.driver.page_source)
                        print(f"   é é¢å…§å®¹é•·åº¦: {page_length} å­—å…ƒ")
                        
                        if page_length > 10000:  # å¦‚æœé é¢æœ‰è¶³å¤ å…§å®¹
                            # ç›´æ¥æå–åœ–ç‰‡ URLï¼Œä¸é€²è¡Œæ»¾å‹•
                            print("   ğŸ“· ç›´æ¥æŠ“å–åˆå§‹è¼‰å…¥çš„åœ–ç‰‡å…§å®¹")
                            
                            # æå–åœ–ç‰‡ URL
                            image_urls = self.extract_image_urls()
                            
                            if image_urls:
                                print(f"   åœ¨ {url} æ‰¾åˆ° {len(image_urls)} å¼µåœ–ç‰‡")
                                break
                            else:
                                print(f"   åœ¨ {url} æ²’æœ‰æ‰¾åˆ°åœ–ç‰‡ï¼Œå˜—è©¦ä¸‹ä¸€å€‹ URL")
                        else:
                            print(f"   é é¢å…§å®¹å¤ªå°‘ï¼Œå¯èƒ½è¼‰å…¥å¤±æ•—")
                    else:
                        print(f"   é é¢éœ€è¦ç™»å…¥æˆ–æœ‰å…¶ä»–å•é¡Œ")
                        
                except Exception as e:
                    print(f"   è¼‰å…¥ {url} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    continue
            
            if not image_urls:
                print("ğŸ˜” æ‰€æœ‰ URL éƒ½æ²’æœ‰æ‰¾åˆ°åœ–ç‰‡")
                return []
            
            # éæ¿¾é‡è¤‡çš„åœ–ç‰‡
            unique_url_pairs = self.filter_duplicate_urls(image_urls, username)
            
            if not unique_url_pairs:
                print("ğŸ˜” æ‰€æœ‰æ‰¾åˆ°çš„åœ–ç‰‡éƒ½å·²ç¶“ä¸‹è¼‰éäº†")
                return []
            
            # å¦‚æœæ–°åœ–ç‰‡æ•¸é‡ä¸è¶³ï¼Œå˜—è©¦ç²å–æ›´å¤š
            if len(unique_url_pairs) < num_images:
                print(f"âš ï¸  æ–°åœ–ç‰‡æ•¸é‡ ({len(unique_url_pairs)}) å°‘æ–¼éœ€æ±‚ ({num_images})")
                print(f"ğŸ“¥ å°‡ä¸‹è¼‰æ‰€æœ‰ {len(unique_url_pairs)} å¼µæ–°åœ–ç‰‡")
                selected_pairs = unique_url_pairs
            else:
                # éš¨æ©Ÿé¸æ“‡è¦ä¸‹è¼‰çš„åœ–ç‰‡
                selected_pairs = random.sample(unique_url_pairs, num_images)
            
            print(f"ğŸ¯ é¸æ“‡ä¸‹è¼‰ {len(selected_pairs)} å¼µæ–°åœ–ç‰‡")
            
            # ä¸‹è¼‰åœ–ç‰‡
            downloaded_files = []
            url_hashes = []
            
            for i, (url, url_hash) in enumerate(selected_pairs):
                filename = self.generate_filename(username, i, url_hash)
                
                # å¦‚æœæª”ååœ¨é»‘åå–®ä¸­ï¼Œè·³éé€™å¼µåœ–ç‰‡
                if filename is None:
                    print(f"   â­ï¸  è·³éé»‘åå–®åœ–ç‰‡ (index {i})")
                    continue
                    
                file_path = self.download_image(url, filename)
                if file_path:
                    downloaded_files.append(file_path)
                    url_hashes.append(url_hash)
                
                # éš¨æ©Ÿå»¶é²
                time.sleep(random.uniform(1, 3))
            
            print(f"âœ… æˆåŠŸä¸‹è¼‰ {len(downloaded_files)} å¼µæ–°åœ–ç‰‡")
            
            # æ›´æ–° JSON æ™‚å‚³å…¥ URL hashes
            if downloaded_files:
                self.update_images_json(downloaded_files, username, url_hashes)
            
            return downloaded_files
            
        except Exception as e:
            print(f"âŒ æŠ“å–éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
            return []
        
        finally:
            if self.driver:
                self.driver.quit()
                print("ğŸ”’ ç€è¦½å™¨å·²é—œé–‰")
    
    def send_images_to_line_group(self, downloaded_files, username):
        """ç™¼é€å‰›ä¸‹è¼‰çš„åœ–ç‰‡åˆ° LINE ç¾¤çµ„"""
        if not self.channel_access_token or not self.target_id:
            print("âš ï¸ LINE Bot è¨­å®šä¸å®Œæ•´ï¼Œè·³é LINE ç™¼é€")
            print(f"   Channel Access Token: {'è¨­å®š' if self.channel_access_token else 'æœªè¨­å®š'}")
            print(f"   Target ID: {'è¨­å®š' if self.target_id else 'æœªè¨­å®š'}")
            return False
        
        if not downloaded_files:
            print("âš ï¸ æ²’æœ‰ä¸‹è¼‰çš„åœ–ç‰‡å¯ç™¼é€")
            return False
        
        print(f"ğŸ“± æº–å‚™ç™¼é€ {len(downloaded_files)} å¼µ {username} åœ–ç‰‡åˆ° LINE ç¾¤çµ„...")
        
        headers = {
            'Authorization': f'Bearer {self.channel_access_token}',
            'Content-Type': 'application/json'
        }
        
        messages = []
        
        # ç‚ºæ¯å¼µåœ–ç‰‡å»ºç«‹è¨Šæ¯
        for i, file_path in enumerate(downloaded_files):
            filename = os.path.basename(file_path)
            
            # å»ºç«‹ GitHub Pages URL
            if username == 'colne_icol':
                image_url = f"{self.github_pages_base}/colne_icol_images/{filename}"
            elif username == 'daily_combined':
                image_url = f"{self.github_pages_base}/combined_images/{filename}"
            else:
                image_url = f"{self.github_pages_base}/images/{filename}"
            
            print(f"ğŸ”§ DEBUG: åœ–ç‰‡ {i+1} URL: {image_url}")
            
            # æ¸¬è©¦ URL æ˜¯å¦å¯è¨ªå•
            try:
                test_response = requests.head(image_url, timeout=10)
                print(f"   ğŸ§ª URL æ¸¬è©¦: HTTP {test_response.status_code}")
                if test_response.status_code != 200:
                    print(f"   âŒ è­¦å‘Š: åœ–ç‰‡ URL ç„¡æ³•è¨ªå• ({test_response.status_code})")
            except Exception as e:
                print(f"   âŒ è­¦å‘Š: ç„¡æ³•æ¸¬è©¦ URL è¨ªå•æ€§: {e}")
            
            messages.append({
                'type': 'image',
                'originalContentUrl': image_url,
                'previewImageUrl': image_url
            })
        
        # LINE ä¸€æ¬¡æœ€å¤šç™¼é€ 5 å‰‡è¨Šæ¯
        messages = messages[:5]
        
        data = {
            'to': self.target_id,
            'messages': messages
        }
        
        try:
            print("ğŸš€ ç™¼é€ LINE API è«‹æ±‚...")
            response = requests.post(
                'https://api.line.me/v2/bot/message/push',
                headers=headers,
                json=data,
                timeout=30
            )
            
            print(f"ğŸ“¨ LINE API å›æ‡‰: {response.status_code}")
            
            if response.status_code == 200:
                print(f"âœ… æˆåŠŸç™¼é€ {len(messages)} å¼µ {username} åœ–ç‰‡åˆ° LINE ç¾¤çµ„")
                return True
            else:
                print(f"âŒ LINE ç™¼é€å¤±æ•—: {response.status_code} - {response.text}")
                return False
                
        except Exception as e:
            print(f"âŒ LINE ç™¼é€æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False

def generate_random_date_range(days_span=10):
    """ç”Ÿæˆéš¨æ©Ÿçš„æ—¥æœŸå€é–“ï¼ˆ10å¤©ï¼‰"""
    # éš¨æ©Ÿé¸æ“‡ä¸€å€‹é–‹å§‹æ—¥æœŸï¼ˆéå»ä¸€å¹´å…§ï¼‰
    today = datetime.now()
    max_days_back = 365  # æœ€å¤šå¾€å‰ä¸€å¹´
    
    # éš¨æ©Ÿé¸æ“‡é–‹å§‹æ—¥æœŸ
    random_days_back = random.randint(days_span, max_days_back)
    start_date = today - timedelta(days=random_days_back)
    end_date = start_date + timedelta(days=days_span - 1)
    
    start_str = start_date.strftime('%Y-%m-%d')
    end_str = end_date.strftime('%Y-%m-%d')
    
    return start_str, end_str

def daily_scrape_and_send():
    """æ¯æ—¥æŠ“åœ–ã€æäº¤æ¨é€ã€ç™¼é€åˆ° LINE ç¾¤çµ„ - GitHub Actions ç‰ˆæœ¬"""
    print("ğŸ¤– é–‹å§‹æ¯æ—¥æŠ“åœ–ã€æäº¤æ¨é€ã€ç™¼é€åˆ° LINE ç¾¤çµ„...")
    print("=" * 50)
    
    # ç”Ÿæˆéš¨æ©Ÿæ—¥æœŸå€é–“
    start_date, end_date = generate_random_date_range(10)
    print(f"ğŸ² ä½¿ç”¨éš¨æ©Ÿæ—¥æœŸå€é–“: {start_date} ~ {end_date}")
    
    accounts = ['ice_deliverer', 'colne_icol']
    num_images_per_account = 2  # æ¯å€‹å¸³è™ŸæŠ“ 2 å¼µç”¨æ–¼çµ„åˆ
    
    all_downloaded_files = {}  # å„²å­˜æ‰€æœ‰ä¸‹è¼‰çš„æª”æ¡ˆ
    
    # æ­¥é©Ÿ 1: æŠ“å–æ‰€æœ‰å¸³è™Ÿçš„åœ–ç‰‡
    print("\nğŸ”„ æ­¥é©Ÿ 1/3: æŠ“å–åœ–ç‰‡")
    for account in accounts:
        print(f"\nğŸ“· é–‹å§‹è™•ç† {account}...")
        
        # å»ºç«‹çˆ¬èŸ²å¯¦ä¾‹
        scraper = TwitterImageScraperSimple(username=account)
        
        # æŠ“å–åœ–ç‰‡
        downloaded_files = scraper.scrape_user_media(account, num_images_per_account, start_date, end_date)
        
        if downloaded_files:
            print(f"   âœ… {account} æˆåŠŸä¸‹è¼‰ {len(downloaded_files)} å¼µåœ–ç‰‡")
            all_downloaded_files[account] = downloaded_files
        else:
            print(f"   ğŸ˜” {account} æ²’æœ‰ä¸‹è¼‰åˆ°æ–°åœ–ç‰‡")
            all_downloaded_files[account] = []
        
        # å…©å€‹å¸³è™Ÿä¹‹é–“ç­‰å¾…ä¸€ä¸‹
        if account != accounts[-1]:  # ä¸æ˜¯æœ€å¾Œä¸€å€‹å¸³è™Ÿ
            print("   â³ ç­‰å¾… 3 ç§’å¾Œè™•ç†ä¸‹ä¸€å€‹å¸³è™Ÿ...")
            time.sleep(3)
    
    total_new_images = sum(len(files) for files in all_downloaded_files.values())
    
    # æ­¥é©Ÿ 2: å»ºç«‹çµ„åˆåœ–ç‰‡ï¼ˆå¦‚æœæœ‰æ–°åœ–ç‰‡ï¼‰
    print(f"\nğŸ¨ æ­¥é©Ÿ 2/3: å»ºç«‹çµ„åˆåœ–ç‰‡")
    combined_image_path = None
    
    if total_new_images > 0:
        try:
            # å˜—è©¦è¼‰å…¥çµ„åˆåœ–ç‰‡åŠŸèƒ½
            from image_combiner import create_combined_from_new_images
            
            # å»ºç«‹çµ„åˆåœ–ç‰‡
            combined_image_path = create_combined_from_new_images(all_downloaded_files)
            
            if combined_image_path:
                print("   âœ… çµ„åˆåœ–ç‰‡å»ºç«‹æˆåŠŸ")
            else:
                print("   âŒ çµ„åˆåœ–ç‰‡å»ºç«‹å¤±æ•—")
                
        except ImportError:
            print("   âš ï¸ çµ„åˆåœ–ç‰‡æ¨¡çµ„ä¸å­˜åœ¨ï¼Œè·³éçµ„åˆåœ–ç‰‡å»ºç«‹")
        except Exception as e:
            print(f"   âŒ å»ºç«‹çµ„åˆåœ–ç‰‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    else:
        print("   ğŸ˜” æ²’æœ‰æ–°åœ–ç‰‡å¯å»ºç«‹çµ„åˆ")
    
    # æ­¥é©Ÿ 3: ç™¼é€çµ„åˆåœ–ç‰‡åˆ° LINE ç¾¤çµ„ (å¦‚æœæœªè¢«ç¦ç”¨)
    if os.environ.get('SKIP_LINE_SEND', '').lower() == 'true':
        print(f"\nğŸ“± è·³é LINE ç™¼é€ (SKIP_LINE_SEND=true)")
        print(f"\nğŸ‰ æ¯æ—¥ä»»å‹™å®Œæˆï¼")
        print(f"ğŸ“Š ç¸½è¨ˆä¸‹è¼‰ {total_new_images} å¼µåœ–ç‰‡")
        if combined_image_path:
            print(f"ğŸ–¼ï¸ å·²å»ºç«‹çµ„åˆåœ–ç‰‡: {combined_image_path}")
        return True
    
    print(f"\nğŸ“± æ­¥é©Ÿ 3/3: ç™¼é€çµ„åˆåœ–ç‰‡åˆ° LINE ç¾¤çµ„")
    
    total_sent = 0
    if combined_image_path:
        try:
            # ä½¿ç”¨ç¬¬ä¸€å€‹å¸³è™Ÿçš„ scraper ä¾†ç™¼é€çµ„åˆåœ–ç‰‡
            scraper = TwitterImageScraperSimple(username=accounts[0])
            
            # ç™¼é€çµ„åˆåœ–ç‰‡
            if scraper.send_images_to_line_group([combined_image_path], "daily_combined"):
                print("   âœ… çµ„åˆåœ–ç‰‡å·²ç™¼é€åˆ° LINE ç¾¤çµ„")
                total_sent = 1
            else:
                print("   âŒ çµ„åˆåœ–ç‰‡ç™¼é€å¤±æ•—")
                total_sent = 0
                
        except Exception as e:
            print(f"   âŒ ç™¼é€çµ„åˆåœ–ç‰‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            total_sent = 0
    else:
        print("   ğŸ˜” æ²’æœ‰çµ„åˆåœ–ç‰‡å¯ç™¼é€")

    print(f"\nğŸ‰ æ¯æ—¥ä»»å‹™å®Œæˆï¼")
    print(f"ğŸ“Š ç¸½è¨ˆä¸‹è¼‰ {total_new_images} å¼µåœ–ç‰‡")
    print(f"ğŸ“Š ç¸½è¨ˆç™¼é€ {total_sent} å¼µåœ–ç‰‡åˆ° LINE ç¾¤çµ„")
    if combined_image_path:
        print(f"ğŸ–¼ï¸ é¡å¤–å»ºç«‹äº† 1 å¼µçµ„åˆåœ–ç‰‡")
    
    return total_sent > 0

def main():
    """ä¸»å‡½æ•¸"""
    if len(sys.argv) < 2 or sys.argv[1] in ['--help', '-h', 'help']:
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  python web_scraper_simple.py <username> [num_images] [start_date] [end_date]")
        print("  python web_scraper_simple.py daily  # æ¯æ—¥æ¨¡å¼ï¼šå…©å€‹å¸³è™Ÿå„æŠ“2å¼µä¸¦ç™¼é€åˆ°LINE")
        print("\nç¯„ä¾‹:")
        print("  python web_scraper_simple.py ice_deliverer 5")
        print("  python web_scraper_simple.py ice_deliverer 5 2024-01-01 2024-01-31")
        print("  python web_scraper_simple.py ice_deliverer 5 random  (ä½¿ç”¨éš¨æ©Ÿ10å¤©å€é–“)")
        print("  python web_scraper_simple.py daily  (æ¯æ—¥è‡ªå‹•æŠ“åœ–ä¸¦ç™¼LINE)")
        print("\næ”¯æ´çš„å¸³è™Ÿï¼š")
        print("  - ice_deliverer")
        print("  - colne_icol")
        return
    
    # æª¢æŸ¥æ˜¯å¦ç‚ºæ¯æ—¥æ¨¡å¼
    if sys.argv[1].lower() == 'daily':
        daily_scrape_and_send()
        return
    
    username = sys.argv[1]
    
    # é©—è­‰ä½¿ç”¨è€…åç¨±
    valid_usernames = ['ice_deliverer', 'colne_icol']
    if username not in valid_usernames:
        print(f"âŒ ä¸æ”¯æ´çš„ä½¿ç”¨è€…åç¨±: {username}")
        print(f"âœ… æ”¯æ´çš„ä½¿ç”¨è€…åç¨±: {', '.join(valid_usernames)}")
        return
    
    num_images = int(sys.argv[2]) if len(sys.argv) > 2 else 5
    
    # æª¢æŸ¥æ˜¯å¦ä½¿ç”¨éš¨æ©Ÿæ—¥æœŸ
    if len(sys.argv) > 3 and sys.argv[3].lower() == 'random':
        start_date, end_date = generate_random_date_range(10)
        print(f"ğŸ² ä½¿ç”¨éš¨æ©Ÿæ—¥æœŸå€é–“: {start_date} ~ {end_date}")
    else:
        start_date = sys.argv[3] if len(sys.argv) > 3 else None
        end_date = sys.argv[4] if len(sys.argv) > 4 else None
        # å¦‚æœæ²’æœ‰æŒ‡å®šæ—¥æœŸï¼Œé»˜èªä½¿ç”¨éš¨æ©Ÿæ—¥æœŸ
        if not start_date and not end_date:
            start_date, end_date = generate_random_date_range(10)
            print(f"ğŸ² é»˜èªä½¿ç”¨éš¨æ©Ÿæ—¥æœŸå€é–“: {start_date} ~ {end_date}")
    
    scraper = TwitterImageScraperSimple(username=username)
    downloaded_files = scraper.scrape_user_media(username, num_images, start_date, end_date)
    
    if downloaded_files:
        print(f"\nğŸ‰ æŠ“å–å®Œæˆï¼ä¸‹è¼‰äº† {len(downloaded_files)} å¼µåœ–ç‰‡:")
        for file_path in downloaded_files:
            print(f"   ğŸ“¸ {os.path.basename(file_path)}")
        
        # æ›´æ–°å°æ‡‰çš„åœ–ç‰‡æ¸…å–®
        print(f"\nğŸ“ åœ–ç‰‡æ¸…å–®å·²åœ¨ä¸‹è¼‰éç¨‹ä¸­è‡ªå‹•æ›´æ–°")
        
    else:
        print("\nğŸ˜” æ²’æœ‰ä¸‹è¼‰ä»»ä½•åœ–ç‰‡")

if __name__ == '__main__':
    main()
